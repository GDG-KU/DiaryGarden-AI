from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
)
from datasets import load_dataset
import torch

# -------------------------------------------------------------
# 1) ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------------
MODEL_NAME = "monologg/koelectra-base-v3-discriminator"

# 4ê°œ ê°ì • ë¼ë²¨
label2id = {
    "joy": 0,
    "sadness": 1,
    "anger": 2,
    "neutral": 3,
}
id2label = {v: k for k, v in label2id.items()}

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# -------------------------------------------------------------
# 2) ë°ì´í„°ì…‹ ë¡œë“œ
# -------------------------------------------------------------
# emotion_data.json ìœ„ì¹˜: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€
dataset = load_dataset("json", data_files="emotion_data.json")

# ìë™ train/test split (8:2)
dataset = dataset["train"].train_test_split(test_size=0.2)

# -------------------------------------------------------------
# 3) ì „ì²˜ë¦¬ í•¨ìˆ˜
# -------------------------------------------------------------
def preprocess(batch):
    enc = tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128,
    )
    enc["labels"] = [label2id[x] for x in batch["label"]]
    return enc

dataset = dataset.map(preprocess, batched=True)

# -------------------------------------------------------------
# 4) ëª¨ë¸ ì¤€ë¹„
# -------------------------------------------------------------
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=4,
    label2id=label2id,
    id2label=id2label,
)

# -------------------------------------------------------------
# 5) í•™ìŠµ ì„¤ì •
# -------------------------------------------------------------
training_args = TrainingArguments(
    output_dir="./emotion-model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    fp16=torch.cuda.is_available(),   # GPU ìˆìœ¼ë©´ ìë™ mixed precision
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# -------------------------------------------------------------
# 6) í•™ìŠµ ì‹œì‘
# -------------------------------------------------------------
trainer.train()

# -------------------------------------------------------------
# 7) ê²°ê³¼ ì €ì¥
# -------------------------------------------------------------
model.save_pretrained("./emotion-model")
tokenizer.save_pretrained("./emotion-model")

print("\nğŸ‰ Training Finished! ëª¨ë¸ì´ './emotion-model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
